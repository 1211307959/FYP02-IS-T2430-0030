"""
Ethical Time-Enhanced Revenue Prediction Module

This module provides revenue prediction functionality using an ethical time-enhanced
LightGBM model that eliminates target leakage while maintaining high accuracy (RÂ² = 0.9937).
The model incorporates sophisticated temporal features for accurate sales forecasting
and scenario planning.

Key Features:
- No target leakage: Uses only features available at prediction time
- Advanced temporal modeling: Cyclical encodings, seasonality, holidays
- Price elasticity modeling: Dynamic elasticity based on price ranges
- Production-ready: Comprehensive error handling and validation

Author: Revenue Prediction Team
Version: 2.0 (Ethical Time-Enhanced)
"""

import numpy as np
import pandas as pd
import joblib
import os
from typing import Dict, Any, Union, Optional, List, Tuple
from datetime import datetime

def validate_and_convert_input(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate and convert input data to appropriate types.
    Returns a new dict with validated and converted values.
    """
    validated = {}
    
    # Required fields
    required_fields = ['Unit Price', 'Unit Cost', 'Month', 'Day', 'Weekday', 'Location', '_ProductID', 'Year']
    for field in required_fields:
        if field not in data:
            raise ValueError(f"Missing required field: {field}")
    
    # Valid weekdays
    valid_weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    
    # Convert and validate numeric fields
    try:
        # Convert string inputs to appropriate types
        data['Unit Price'] = float(data['Unit Price'])
        data['Unit Cost'] = float(data['Unit Cost'])
        data['Month'] = int(data['Month'])
        data['Day'] = int(data['Day'])
        data['Year'] = int(data['Year'])
        # _ProductID: always treat as string
        data['_ProductID'] = str(data['_ProductID'])
        
        # Validate numeric ranges
        if not 1 <= data['Month'] <= 12:
            raise ValueError("Month must be between 1 and 12")
        if not 1 <= data['Day'] <= 31:
            raise ValueError("Day must be between 1 and 31")
        if data['Unit Price'] < 0:
            raise ValueError("Unit Price cannot be negative")
        if data['Unit Cost'] < 0:
            raise ValueError("Unit Cost cannot be negative")
        if data['Unit Cost'] > data['Unit Price']:
            raise ValueError("Unit Cost cannot be greater than Unit Price")
        
        # Validate weekday
        if isinstance(data['Weekday'], str) and data['Weekday'] not in valid_weekdays:
            raise ValueError(f"Weekday must be one of: {', '.join(valid_weekdays)}")
        
    except (ValueError, TypeError) as e:
        if isinstance(e, ValueError):
            raise ValueError(str(e))
        raise ValueError(f"Invalid numeric value: {str(e)}")
    
    return data

def get_available_locations_and_products() -> Tuple[List[str], List[int]]:
    """
    Dynamically load available locations and products from ALL CSV files in the data folder.
    This ensures the system can adapt to new data without hardcoded values.
    
    Returns:
        Tuple containing:
        - locations: List of available location names
        - products: List of available product IDs
    """
    try:
        import pandas as pd
        import os
        
        # Load from ALL CSV files in the data directory (same as other functions)
        data_dir = 'public/data'
        if os.path.exists(data_dir):
            csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
            
            if csv_files:
                all_locations = set()
                all_products = set()
                
                for file in csv_files:
                    file_path = os.path.join(data_dir, file)
                    try:
                        df = pd.read_csv(file_path)
                        if 'Location' in df.columns:
                            all_locations.update(df['Location'].unique())
                        if '_ProductID' in df.columns:
                            all_products.update(df['_ProductID'].unique())
                    except Exception as e:
                        print(f"Warning: Could not load {file}: {e}")
                        continue
                
                if all_locations and all_products:
                    # Filter out any null/empty values
                    locations = [loc for loc in all_locations if loc and not pd.isna(loc)]
                    products = [prod for prod in all_products if prod and not pd.isna(prod)]
                    
                    locations = sorted(locations)
                    products = sorted(products)
                    print(f"Loaded from data folder: {len(locations)} locations, {len(products)} products")
                    return locations, products
        
        # Fallback to trainingdataset.csv if data folder approach fails
        dataset_path = 'trainingdataset.csv'
        if os.path.exists(dataset_path):
            df = pd.read_csv(dataset_path)
            
            # Get unique locations and products
            locations = df['Location'].unique().tolist() if 'Location' in df.columns else []
            products = df['_ProductID'].unique().tolist() if '_ProductID' in df.columns else []
            
            # Filter out any null/empty values
            locations = [loc for loc in locations if loc and not pd.isna(loc)]
            products = [prod for prod in products if prod and not pd.isna(prod)]
            
            locations = sorted(locations)
            products = sorted(products)
            print(f"Loaded from trainingdataset.csv: {len(locations)} locations, {len(products)} products")
            return locations, products
        else:
            # Fallback to encoders if dataset not available
            try:
                encoders_path = 'revenue_encoders_time_enhanced_ethical.pkl'
                encoders = joblib.load(encoders_path)
                locations = list(encoders.get('Location', {}).classes_) if 'Location' in encoders else []
                products = list(encoders.get('_ProductID', {}).classes_) if '_ProductID' in encoders else []
                print(f"Loaded from encoders: {len(locations)} locations, {len(products)} products")
                return locations, products
            except:
                # Final fallback to known values
                print("Using fallback values: 5 locations, 47 products")
                return ['Central', 'East', 'North', 'South', 'West'], list(range(1, 48))
                
    except Exception as e:
        print(f"Warning: Could not load locations/products dynamically: {str(e)}")
        # Return known fallback values
        print("Using fallback values: 5 locations, 47 products")
        return ['Central', 'East', 'North', 'South', 'West'], list(range(1, 48))

def load_model() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    """
    Load the trained ethical time-enhanced model and associated data files.
    
    Returns:
        Tuple[Dict, Dict, Dict]: A tuple containing:
            - model_data: The trained LightGBM model and metadata
            - encoders: Label encoders for categorical variables
            - reference_data: Reference statistics for feature engineering
    
    Raises:
        FileNotFoundError: If model files are not found
        RuntimeError: If there's an error loading the model files
        
    Example:
        >>> model_data, encoders, reference_data = load_model()
        >>> print(f"Model loaded successfully: {model_data.get('version', 'unknown')}")
    """
    model_path = 'revenue_model_time_enhanced_ethical.pkl'
    encoders_path = 'revenue_encoders_time_enhanced_ethical.pkl'
    reference_path = 'reference_data_time_enhanced_ethical.pkl'
    
    if not os.path.exists(model_path) or not os.path.exists(encoders_path):
        raise FileNotFoundError(
            "Ethical time-enhanced model files not found. Please run train_time_enhanced_ethical_model.py first."
        )
    
    try:
        model_data = joblib.load(model_path)
        encoders = joblib.load(encoders_path)
        reference_data = joblib.load(reference_path) if os.path.exists(reference_path) else {}
        return model_data, encoders, reference_data
    except Exception as e:
        raise RuntimeError(f"Error loading model files: {str(e)}")

def add_enhanced_time_features(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Add enhanced time-based features to the input data.
    """
    # Create a proper date object
    try:
        year = data['Year']
        month = data['Month']
        day = min(data['Day'], 28)  # Clip to avoid month end issues
        date = datetime(year, month, day)
        
        # Add day of year (1-366)
        data['Day_of_Year'] = date.timetuple().tm_yday
        
        # Add week of year (1-53)
        data['Week_of_Year'] = date.isocalendar()[1]
        
        # Cyclical encoding of time
        data['Month_Sin'] = np.sin(2 * np.pi * month / 12)
        data['Month_Cos'] = np.cos(2 * np.pi * month / 12)
        data['Day_Sin'] = np.sin(2 * np.pi * day / 31)
        data['Day_Cos'] = np.cos(2 * np.pi * day / 31)
        data['Day_of_Year_Sin'] = np.sin(2 * np.pi * data['Day_of_Year'] / 366)
        data['Day_of_Year_Cos'] = np.cos(2 * np.pi * data['Day_of_Year'] / 366)
        data['Week_of_Year_Sin'] = np.sin(2 * np.pi * data['Week_of_Year'] / 53)
        data['Week_of_Year_Cos'] = np.cos(2 * np.pi * data['Week_of_Year'] / 53)
        
        # Quarter
        data['Quarter'] = (month - 1) // 3 + 1
        
        # Seasons (Northern Hemisphere)
        data['Is_Winter'] = 1 if month in [12, 1, 2] else 0
        data['Is_Spring'] = 1 if month in [3, 4, 5] else 0
        data['Is_Summer'] = 1 if month in [6, 7, 8] else 0
        data['Is_Fall'] = 1 if month in [9, 10, 11] else 0
        
        # Holiday season (Nov-Dec)
        data['Is_Holiday_Season'] = 1 if month in [11, 12] else 0
        
        # Weekend
        if isinstance(data['Weekday'], str):
            data['Is_Weekend'] = 1 if data['Weekday'] in ['Saturday', 'Sunday'] else 0
        else:
            weekday_num = data.get('Weekday_Numeric', data['Weekday'])
            data['Is_Weekend'] = 1 if weekday_num >= 5 else 0
        
        # Holiday flags
        holidays = [
            # USA holidays - simplified dates
            (1, 1),    # New Year's Day
            (7, 4),    # Independence Day
            (12, 25),  # Christmas
            (11, 25),  # Thanksgiving-ish (approximation)
            (5, 30),   # Memorial Day-ish (approximation)
            (9, 5),    # Labor Day-ish (approximation)
            (2, 14),   # Valentine's Day
            (10, 31),  # Halloween
        ]
        data['Is_Holiday'] = 1 if (month, day) in holidays else 0
        
        return data
        
    except Exception as e:
        raise ValueError(f"Error adding enhanced time features: {str(e)}")

def preprocess(data: Dict[str, Any], model_data: Dict[str, Any], encoders: Dict[str, Any], reference_data: Dict[str, Any]) -> pd.DataFrame:
    """
    Preprocess a single data point for prediction.
    No target leakage - uses only features known before a sale.
    """
    try:
        # Start with basic input features
        processed = pd.DataFrame([data])
        
        # Add enhanced time features
        for key, value in add_enhanced_time_features(data).items():
            if key not in processed.columns:
                processed[key] = value
        
        # Encode categorical variables
        for col, encoder in encoders.items():
            if col in processed.columns:
                try:
                    if col == 'Location':
                        # Handle unknown locations
                        if processed[col].iloc[0] not in encoder.classes_:
                            raise ValueError(f"Unknown location: {processed[col].iloc[0]}")
                        processed[f'{col}_Encoded'] = encoder.transform(processed[col])
                    elif col == '_ProductID':
                        # Handle unknown product IDs
                        if processed[col].iloc[0] not in encoder.classes_:
                            raise ValueError(f"Unknown product ID: {processed[col].iloc[0]}")
                        processed[f'ProductID_Encoded'] = encoder.transform(processed[col])
                    elif col == 'Weekday':
                        # Handle weekday encoding
                        weekday_map = encoders['Weekday']
                        weekday = str(processed[col].iloc[0])
                        if weekday in weekday_map:
                            processed['Weekday_Numeric'] = weekday_map[weekday]
                        else:
                            # Default to Wednesday (3) for unknown weekdays
                            processed['Weekday_Numeric'] = 3
                except Exception as e:
                    raise ValueError(f"Error encoding {col}: {str(e)}")
        
        # Basic price features
        processed['Price_to_Cost_Ratio'] = processed['Unit Price'] / processed['Unit Cost']
        processed['Margin_Per_Unit'] = processed['Unit Price'] - processed['Unit Cost']
        processed['Margin_Per_Unit_Pct'] = (processed['Margin_Per_Unit'] / processed['Unit Price']) * 100
        processed['Price_Squared'] = processed['Unit Price'] ** 2
        processed['Price_Log'] = np.log1p(processed['Unit Price'])
        
        # Use reference data for more complex features if available
        product_id = str(data['_ProductID'])
        location = data['Location']
        
        # Product price statistics from reference data
        if 'product_price_avg' in reference_data and product_id in reference_data['product_price_avg']:
            product_avg_price = reference_data['product_price_avg'][product_id]
        else:
            product_avg_price = data['Unit Price']
            
        processed['Product_Unit Price_mean'] = product_avg_price
        processed['Product_Unit Price_std'] = 0
        processed['Product_Unit Price_min'] = product_avg_price * 0.8
        processed['Product_Unit Price_max'] = product_avg_price * 1.2
        
        # Product cost statistics
        if 'product_cost_avg' in reference_data and product_id in reference_data['product_cost_avg']:
            product_avg_cost = reference_data['product_cost_avg'][product_id]
        else:
            product_avg_cost = data['Unit Cost']
            
        processed['Product_Unit Cost_mean'] = product_avg_cost
        
        # Product popularity (default to medium popularity)
        processed['Product_Popularity'] = 1000
        
        # Location price statistics
        if 'location_price_avg' in reference_data and location in reference_data['location_price_avg']:
            location_avg_price = reference_data['location_price_avg'][location]
        else:
            location_avg_price = data['Unit Price']
            
        processed['Location_Unit Price_mean'] = location_avg_price
        processed['Location_Unit Price_std'] = 0
        processed['Location_Unit Price_min'] = location_avg_price * 0.8
        processed['Location_Unit Price_max'] = location_avg_price * 1.2
        
        # Location cost statistics
        if 'location_cost_avg' in reference_data and location in reference_data['location_cost_avg']:
            location_avg_cost = reference_data['location_cost_avg'][location]
        else:
            location_avg_cost = data['Unit Cost']
            
        processed['Location_Unit Cost_mean'] = location_avg_cost
        
        # Product-Month interactions (using month-specific data if available)
        month = data.get('Month', 1)
        quarter = ((month - 1) // 3) + 1
        is_weekend = 1 if data.get('Weekday', 'Monday') in ['Saturday', 'Sunday'] else 0
        
        # Get month-specific product price
        product_month_key = f"{product_id}_{month}"
        if 'product_month_price_stats' in reference_data and product_month_key in reference_data['product_month_price_stats']:
            product_month_price = reference_data['product_month_price_stats'][product_month_key]['mean']
        else:
            product_month_price = product_avg_price
        processed['Product_Month_Unit Price_mean'] = product_month_price
        
        # Product-Quarter interactions
        product_quarter_key = f"{product_id}_{quarter}"
        if 'product_quarter_price_stats' in reference_data and product_quarter_key in reference_data['product_quarter_price_stats']:
            product_quarter_price = reference_data['product_quarter_price_stats'][product_quarter_key]['mean']
        else:
            product_quarter_price = product_avg_price
        processed['Product_Quarter_Unit Price_mean'] = product_quarter_price
        
        # Location-Month interactions
        location_month_key = f"{location}_{month}"
        if 'location_month_price_stats' in reference_data and location_month_key in reference_data['location_month_price_stats']:
            location_month_price = reference_data['location_month_price_stats'][location_month_key]['mean']
        else:
            location_month_price = location_avg_price
        processed['Location_Month_Unit Price_mean'] = location_month_price
        
        # Weekend-Location interactions
        location_weekend_key = f"{location}_{is_weekend}"
        if 'location_weekend_price_stats' in reference_data and location_weekend_key in reference_data['location_weekend_price_stats']:
            location_weekend_price = reference_data['location_weekend_price_stats'][location_weekend_key]
        else:
            location_weekend_price = location_avg_price
        processed['Location_Weekend_Price_mean'] = location_weekend_price
        
        # Product-Weekend interactions
        product_weekend_key = f"{product_id}_{is_weekend}"
        if 'product_weekend_price_stats' in reference_data and product_weekend_key in reference_data['product_weekend_price_stats']:
            product_weekend_price = reference_data['product_weekend_price_stats'][product_weekend_key]
        else:
            product_weekend_price = product_avg_price
        processed['Product_Weekend_Price_mean'] = product_weekend_price
        
        # Price comparison features - NOW USING PROPER SEASONAL DATA
        processed['Price_vs_Product_Avg'] = processed['Unit Price'] / product_avg_price if product_avg_price > 0 else 1.0
        processed['Price_vs_Location_Avg'] = processed['Unit Price'] / location_avg_price if location_avg_price > 0 else 1.0
        processed['Price_Seasonal_Deviation'] = processed['Unit Price'] / product_month_price if product_month_price > 0 else 1.0
        
        # Feature interactions
        processed['Price_Popularity'] = processed['Unit Price'] * processed['Product_Popularity']
        processed['Price_Location'] = processed['Unit Price'] * processed['Location_Unit Price_mean']
        processed['Price_Month'] = processed['Unit Price'] * processed['Month']
        processed['Price_Quarter'] = processed['Unit Price'] * processed['Quarter']
        processed['Price_Holiday'] = processed['Unit Price'] * processed['Is_Holiday']
        processed['Price_Weekend'] = processed['Unit Price'] * processed['Is_Weekend']
        
        # Get model features
        model_features = None
        if 'features' in model_data:
            model_features = model_data['features']
        elif 'feature_names' in model_data:
            model_features = model_data['feature_names']
            
        if not model_features:
            # If no explicit feature list, return all processed features
            return processed
        
        # Initialize missing features to 0
        for feature in model_features:
            if feature not in processed.columns:
                processed[feature] = 0
        
        # Select and order features according to the model
        processed = processed[model_features]
        
        return processed
        
    except Exception as e:
        raise ValueError(f"Error in preprocessing: {str(e)}")

def predict_revenue(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Predict revenue using the ethical time-enhanced LightGBM model.
    
    This function provides comprehensive revenue prediction with automatic handling
    of location aggregation, advanced price elasticity modeling, and derived metrics
    calculation. The model maintains strict ethical standards by using only features
    available at prediction time.
    
    Args:
        data (Dict[str, Any]): Input data dictionary containing:
            - Unit Price (float): Product unit price
            - Unit Cost (float): Product unit cost
            - _ProductID (str/int): Product identifier
            - Location (str): Location name or "All" for aggregation
            - Month (int): Month (1-12)
            - Day (int): Day of month (1-31)
            - Weekday (str): Day name (e.g., "Monday")
            - Year (int): Year
    
    Returns:
        Dict[str, Any]: Prediction results containing:
            - predicted_revenue (float): Predicted revenue amount
            - estimated_quantity (int): Estimated quantity sold
            - total_cost (float): Total cost for the transaction
            - profit (float): Estimated profit
            - profit_margin_pct (float): Profit margin percentage
            - price_ratio (float): Price relative to product average
            - location_count (int, optional): Number of locations aggregated
            - note (str, optional): Additional information about prediction
    
    Raises:
        ValueError: If input data is invalid or missing required fields
        FileNotFoundError: If model files are not found
        RuntimeError: If prediction fails
        
    Example:
        >>> input_data = {
        ...     "Unit Price": 150.0,
        ...     "Unit Cost": 75.0,
        ...     "_ProductID": "12",
        ...     "Location": "North",
        ...     "Month": 6,
        ...     "Day": 15,
        ...     "Weekday": "Friday",
        ...     "Year": 2023
        ... }
        >>> result = predict_revenue(input_data)
        >>> print(f"Revenue: ${result['predicted_revenue']:.2f}")
        Revenue: $9750.00
    """
    try:
        # Create a copy of data to avoid modifying the original
        data = data.copy()
        
        # Check for special "All" location case for aggregation
        is_aggregating_locations = False
        if data.get('Location') == 'All' or data.get('aggregate_locations'):
            is_aggregating_locations = True
            
        # Load the model
        model_data, encoders, reference_data = load_model()
        model = model_data['model']
        
        # Get feature names - handle different model structures
        feature_names = None
        if 'features' in model_data:
            feature_names = model_data['features']
        elif 'feature_names' in model_data:
            feature_names = model_data['feature_names']
        elif hasattr(model, 'feature_name_'):
            feature_names = model.feature_name_
        elif hasattr(model, 'feature_names_'):
            feature_names = model.feature_names_
        
        if is_aggregating_locations:
            # Get all available locations from the location encoder
            all_locations = encoders.get('Location').classes_
            
            # Aggregate predictions across all locations
            predictions = []
            for location in all_locations:
                # Create a copy with this specific location
                location_data = data.copy()
                location_data['Location'] = location
                
                try:
                    # Validate and convert input
                    validated_data = validate_and_convert_input(location_data)
                    
                    # Important: Generate location-specific model features
                    # This is what was missing before - we need to regenerate all features for each location
                    processed = preprocess(validated_data, model_data, encoders, reference_data)
                    
                    # Select required features
                    if feature_names:
                        # Get all available features
                        missing_features = set(feature_names) - set(processed.columns)
                        if missing_features:
                            for feature in missing_features:
                                processed[feature] = 0
                        
                        X = processed[feature_names]
                    else:
                        # If no feature names are available, use all columns
                        X = processed
                    
                    # Predict revenue (model returns log-transformed values)
                    y_pred_log = model.predict(X)[0]
                    
                    # Apply inverse log transformation to get actual revenue
                    y_pred = np.expm1(y_pred_log)
                    
                    # Apply price bounds check and enhanced price elasticity
                    unit_price = float(validated_data['Unit Price'])
                    unit_cost = float(validated_data['Unit Cost'])
                    
                    # Get product reference price if available
                    product_id = validated_data.get('_ProductID')
                    product_avg_price = None
                    
                    # Debug: Check product_id type and existence
                    if product_id is not None:
                        # Ensure product_id is integer
                        product_id = int(product_id)
                        
                    if 'product_price_avg' in reference_data and product_id in reference_data['product_price_avg']:
                        product_avg_price = reference_data['product_price_avg'][product_id]
                    else:
                        # Use all products average as fallback (much more realistic for this dataset)
                        if 'product_price_avg' in reference_data:
                            product_avg_price = sum(reference_data['product_price_avg'].values()) / len(reference_data['product_price_avg'])
                        else:
                            product_avg_price = 5000.0  # Realistic fallback based on dataset
                    
                    # Apply enhanced price elasticity
                    # Calculate price ratio compared to average price for this product
                    price_ratio = unit_price / product_avg_price if product_avg_price > 0 else unit_price / 100.0
                    
                    # Apply stronger elasticity for higher prices
                    if price_ratio > 1.0:
                        # Higher than average price - apply stronger elasticity
                        # Exponential decay as price increases
                        elasticity_factor = np.exp(-0.5 * (price_ratio - 1.0))
                        y_pred = y_pred * elasticity_factor
                        
                        # For very high prices, apply even stronger elasticity
                        if price_ratio > 3.0:
                            additional_factor = np.exp(-1.0 * (price_ratio - 3.0))
                            y_pred = y_pred * additional_factor
                    
                    # For extreme prices, adjust prediction
                    if unit_price > 100000:  # Upper price bound check
                        y_pred = 0
                    
                    # Convert to quantity with realistic rounding
                    # Ensure quantity decreases as price increases
                    if unit_price > 0:
                        # Calculate raw quantity
                        raw_quantity = y_pred / unit_price
                        
                        # Apply additional price-based quantity adjustment
                        # Higher prices should result in lower quantities with more variability
                        if price_ratio > 1.5:
                            # Apply stronger quantization for higher prices
                            # This ensures we get more distinct quantity values
                            if raw_quantity < 10:
                                predicted_quantity = max(0, int(raw_quantity))
                            else:
                                predicted_quantity = max(0, int(raw_quantity / 5) * 5)
                        else:
                            # Normal quantization for regular prices
                            predicted_quantity = max(0, round(raw_quantity))
                    else:
                        predicted_quantity = 0
                    
                    # KEEP ORIGINAL ML REVENUE PREDICTION - Don't recalculate based on quantity
                    # The ML model's revenue prediction is more accurate than quantity*price
                    adjusted_revenue = y_pred
                    
                    # Calculate derived metrics
                    total_cost = predicted_quantity * unit_cost
                    profit = adjusted_revenue - total_cost
                    profit_margin_pct = (profit / adjusted_revenue) * 100 if adjusted_revenue > 0 else 0
                    
                    # Add to predictions list
                    predictions.append({
                        'location': location,
                        'predicted_revenue': adjusted_revenue,
                        'estimated_quantity': predicted_quantity,
                        'total_cost': total_cost,
                        'profit': profit
                    })
                    
                    # Reduced logging for performance - individual location predictions not shown
                    
                except Exception as e:
                    # Skip this location if there was an error
                    print(f"Error predicting for location {location}: {str(e)}")
            
            # Aggregate results
            if not predictions:
                raise ValueError("Failed to generate predictions for any location")
                
            total_revenue = sum(p['predicted_revenue'] for p in predictions)
            total_quantity = sum(p['estimated_quantity'] for p in predictions)
            total_cost = sum(p['total_cost'] for p in predictions)
            total_profit = sum(p['profit'] for p in predictions)
            
            # Calculate aggregated profit margin
            profit_margin_pct = (total_profit / total_revenue) * 100 if total_revenue > 0 else 0
            
            # Create season info from the time features
            month = int(data['Month'])
            seasons = {
                (12, 1, 2): 'Winter',
                (3, 4, 5): 'Spring',
                (6, 7, 8): 'Summer',
                (9, 10, 11): 'Fall'
            }
            season = next((s for m_range, s in seasons.items() if month in m_range), 'Unknown')
            
            # Return aggregated results
            return {
                'predicted_revenue': total_revenue,
                'revenue': total_revenue,  # Include both field names for compatibility
                'estimated_quantity': total_quantity,
                'predicted_quantity': total_quantity,  # Include both field names
                'total_cost': total_cost,
                'profit': total_profit,
                'profit_margin_pct': profit_margin_pct,
                'unit_price': float(data['Unit Price']),
                'unit_cost': float(data['Unit Cost']),
                'month': int(data['Month']),
                'day': int(data['Day']),
                'weekday': data['Weekday'],
                'season': season,
                'time_features': add_enhanced_time_features(data),
                'model_type': 'ethical_time_enhanced',
                'locations_aggregated': True,
                'location_count': len(predictions)
            }
        else:
            # Handle single location prediction as before
            # Validate and convert input
            validated_data = validate_and_convert_input(data)
            
            # Preprocess data
            processed = preprocess(validated_data, model_data, encoders, reference_data)
            
            # Select required features
            if feature_names:
                # Get all available features
                missing_features = set(feature_names) - set(processed.columns)
                if missing_features:
                    for feature in missing_features:
                        processed[feature] = 0
                
                X = processed[feature_names]
            else:
                # If no feature names are available, use all columns
                X = processed
            
            # Predict revenue (model returns log-transformed values)
            y_pred_log = model.predict(X)[0]
            
            # Apply inverse log transformation to get actual revenue
            y_pred = np.expm1(y_pred_log)
            
            # Apply price bounds check and enhanced price elasticity
            unit_price = float(validated_data['Unit Price'])
            unit_cost = float(validated_data['Unit Cost'])
            
            # Get product reference price if available
            product_id = validated_data.get('_ProductID')
            product_avg_price = None
            
            # Debug: Check product_id type and existence
            if product_id is not None:
                # Ensure product_id is integer
                product_id = int(product_id)
                
            if 'product_price_avg' in reference_data and product_id in reference_data['product_price_avg']:
                product_avg_price = reference_data['product_price_avg'][product_id]
                # Reduced logging for performance - product price lookup
            else:
                # Use all products average as fallback (much more realistic for this dataset)
                if 'product_price_avg' in reference_data:
                    product_avg_price = sum(reference_data['product_price_avg'].values()) / len(reference_data['product_price_avg'])
                    print(f"Using overall average price for product {product_id}: ${product_avg_price:.2f}")
                else:
                    product_avg_price = 5000.0  # Realistic fallback based on dataset
                    print(f"Using default fallback price for product {product_id}: ${product_avg_price:.2f}")
            
            # Apply enhanced price elasticity
            # Calculate price ratio compared to average price for this product
            price_ratio = unit_price / product_avg_price if product_avg_price > 0 else unit_price / 100.0
            
            # Apply stronger elasticity for higher prices
            if price_ratio > 1.0:
                # Higher than average price - apply stronger elasticity
                # Exponential decay as price increases
                elasticity_factor = np.exp(-0.5 * (price_ratio - 1.0))
                y_pred = y_pred * elasticity_factor
                
                # For very high prices, apply even stronger elasticity
                if price_ratio > 3.0:
                    additional_factor = np.exp(-1.0 * (price_ratio - 3.0))
                    y_pred = y_pred * additional_factor
            
            # For extreme prices, adjust prediction
            if unit_price > 100000:  # Upper price bound check
                y_pred = 0
            
            # Convert to quantity with realistic rounding
            # Ensure quantity decreases as price increases
            if unit_price > 0:
                # Calculate raw quantity
                raw_quantity = y_pred / unit_price
                
                # Apply additional price-based quantity adjustment
                # Higher prices should result in lower quantities with more variability
                if price_ratio > 1.5:
                    # Apply stronger quantization for higher prices
                    # This ensures we get more distinct quantity values
                    if raw_quantity < 10:
                        predicted_quantity = max(0, int(raw_quantity))
                    else:
                        predicted_quantity = max(0, int(raw_quantity / 5) * 5)
                else:
                    # Normal quantization for regular prices (keeping original logic for compatibility)
                    predicted_quantity = max(0, round(raw_quantity))
            else:
                predicted_quantity = 0
            
            # KEEP ORIGINAL ML REVENUE PREDICTION - Don't recalculate based on quantity  
            # The ML model's revenue prediction is more accurate than quantity*price
            adjusted_revenue = y_pred
            
            # Calculate derived metrics
            total_cost = predicted_quantity * unit_cost
            profit = adjusted_revenue - total_cost
            profit_margin_pct = (profit / adjusted_revenue) * 100 if adjusted_revenue > 0 else 0
            
            # Create season info from the time features
            month = int(data['Month'])
            seasons = {
                (12, 1, 2): 'Winter',
                (3, 4, 5): 'Spring',
                (6, 7, 8): 'Summer',
                (9, 10, 11): 'Fall'
            }
            season = next((s for m_range, s in seasons.items() if month in m_range), 'Unknown')
            
            # Return results with both field naming conventions for compatibility
            return {
                'predicted_revenue': adjusted_revenue,
                'revenue': adjusted_revenue,  # Include both field names for compatibility
                'estimated_quantity': predicted_quantity,
                'predicted_quantity': predicted_quantity,  # Include both field names
                'total_cost': total_cost,
                'profit': profit,
                'profit_margin_pct': profit_margin_pct,
                'unit_price': unit_price,
                'unit_cost': unit_cost,
                'month': int(data['Month']),
                'day': int(data['Day']),
                'weekday': data['Weekday'],
                'season': season,
                'time_features': add_enhanced_time_features(data),
                'model_type': 'ethical_time_enhanced',
                'price_ratio': price_ratio
            }
    
    except Exception as e:
        # Log error and return error message
        import traceback
        traceback.print_exc()
        return {'error': str(e)}

def predict_revenue_for_forecasting(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Predict revenue specifically for forecasting use cases.
    Uses direct revenue prediction without quantity rounding to preserve time variations.
    
    This function is designed for sales forecasting where we want to maintain
    small day-to-day variations that would be lost in quantity rounding.
    
    Supports 'All' location aggregation by summing predictions across all locations.
    """
    try:
        # Check if "All" location is specified - handle aggregation
        location = data.get('Location', 'North')
        
        if location == 'All':
            # Get all unique locations from reference data
            model_data, encoders, reference_data = load_model()
            
            # Get available locations from the location encoder
            all_locations = []
            if 'Location' in encoders and hasattr(encoders['Location'], 'classes_'):
                all_locations = list(encoders['Location'].classes_)
            else:
                # Fallback to common locations
                all_locations = ['North', 'Central', 'South', 'East', 'West']
            
            # Remove 'All' from the list if it exists (to avoid recursion)
            if 'All' in all_locations:
                all_locations.remove('All')
            
            # Make predictions for each location and sum them
            total_revenue = 0
            total_quantity = 0
            total_cost = 0
            total_profit = 0
            successful_locations = 0
            
            for loc in all_locations:
                # Create data for this specific location
                location_data = data.copy()
                location_data['Location'] = loc
                
                # Make prediction for this location (frequency scaling will be applied automatically)
                location_result = predict_revenue_for_forecasting(location_data)
                
                if 'error' not in location_result:
                    total_revenue += location_result.get('predicted_revenue', 0)
                    total_quantity += location_result.get('estimated_quantity', 0)
                    total_cost += location_result.get('total_cost', 0)
                    total_profit += location_result.get('profit', 0)
                    successful_locations += 1
            
            if successful_locations == 0:
                raise ValueError("No successful predictions for any location")
            
            # Calculate aggregated metrics
            profit_margin_pct = (total_profit / total_revenue) * 100 if total_revenue > 0 else 0
            
            # Create season info
            month = int(data.get('Month', 6))
            seasons = {
                (12, 1, 2): 'Winter',
                (3, 4, 5): 'Spring',
                (6, 7, 8): 'Summer',
                (9, 10, 11): 'Fall'
            }
            season = next((s for m_range, s in seasons.items() if month in m_range), 'Unknown')
            
            # Return aggregated results
            return {
                'predicted_revenue': total_revenue,
                'revenue': total_revenue,
                'estimated_quantity': total_quantity,
                'predicted_quantity': total_quantity,
                'total_cost': total_cost,
                'profit': total_profit,
                'profit_margin_pct': profit_margin_pct,
                'unit_price': float(data.get('Unit Price', 0)),
                'unit_cost': float(data.get('Unit Cost', 0)),
                'month': month,
                'day': int(data.get('Day', 1)),
                'weekday': data.get('Weekday', 'Monday'),
                'season': season,
                'time_features': add_enhanced_time_features(data),
                'model_type': 'ethical_time_enhanced_forecasting_aggregated',
                'direct_revenue': True,
                'location_count': successful_locations,
                'locations_aggregated': True,
                'note': f'Aggregated forecast across {successful_locations} locations'
            }
        
        # Single location prediction (original logic)
        # Load model and dependencies
        model_data, encoders, reference_data = load_model()
        model = model_data['model']
        
        # Validate and convert input data
        validated_data = validate_and_convert_input(data)
        
        # Preprocess the data
        processed = preprocess(validated_data, model_data, encoders, reference_data)
        
        # Select features for prediction
        feature_names = model_data.get('features', [])
        if feature_names:
            # Get all available features
            missing_features = set(feature_names) - set(processed.columns)
            if missing_features:
                for feature in missing_features:
                    processed[feature] = 0
            
            X = processed[feature_names]
        else:
            # If no feature names are available, use all columns
            X = processed
        
        # Predict revenue (model returns log-transformed values)
        y_pred_log = model.predict(X)[0]
        
        # Apply inverse log transformation to get actual revenue
        y_pred = np.expm1(y_pred_log)
        
        # Apply price bounds check and enhanced price elasticity
        unit_price = float(validated_data['Unit Price'])
        unit_cost = float(validated_data['Unit Cost'])
        
        # Get product reference price if available
        product_id = validated_data.get('_ProductID')
        product_avg_price = None
        
        # Debug: Check product_id type and existence
        if product_id is not None:
            # Ensure product_id is integer
            product_id = int(product_id)
            
        if 'product_price_avg' in reference_data and product_id in reference_data['product_price_avg']:
            product_avg_price = reference_data['product_price_avg'][product_id]
        else:
            # Use all products average as fallback
            if 'product_price_avg' in reference_data:
                product_avg_price = sum(reference_data['product_price_avg'].values()) / len(reference_data['product_price_avg'])
            else:
                product_avg_price = 5000.0  # Realistic fallback based on dataset
        
        # Apply enhanced price elasticity
        # Calculate price ratio compared to average price for this product
        price_ratio = unit_price / product_avg_price if product_avg_price > 0 else unit_price / 100.0
        
        # Apply stronger elasticity for higher prices
        if price_ratio > 1.0:
            # Higher than average price - apply stronger elasticity
            # Exponential decay as price increases
            elasticity_factor = np.exp(-0.5 * (price_ratio - 1.0))
            y_pred = y_pred * elasticity_factor
            
            # For very high prices, apply even stronger elasticity
            if price_ratio > 3.0:
                additional_factor = np.exp(-1.0 * (price_ratio - 3.0))
                y_pred = y_pred * additional_factor
        
        # For extreme prices, adjust prediction
        if unit_price > 100000:  # Upper price bound check
            y_pred = 0
        
        # USE DIRECT REVENUE (preserve time variations)
        direct_revenue = max(0, y_pred)
        
        # Calculate quantity from revenue for display purposes (not used for revenue calculation)
        display_quantity = direct_revenue / unit_price if unit_price > 0 else 0
        
        # Calculate derived metrics based on direct revenue
        total_cost = display_quantity * unit_cost
        profit = direct_revenue - total_cost
        profit_margin_pct = (profit / direct_revenue) * 100 if direct_revenue > 0 else 0
        
        # Create season info from the time features
        month = int(data['Month'])
        seasons = {
            (12, 1, 2): 'Winter',
            (3, 4, 5): 'Spring',
            (6, 7, 8): 'Summer',
            (9, 10, 11): 'Fall'
        }
        season = next((s for m_range, s in seasons.items() if month in m_range), 'Unknown')
        
        # Return results with both field naming conventions for compatibility
        return {
            'predicted_revenue': direct_revenue,
            'revenue': direct_revenue,  # Include both field names for compatibility
            'estimated_quantity': display_quantity,
            'predicted_quantity': display_quantity,  # Include both field names
            'total_cost': total_cost,
            'profit': profit,
            'profit_margin_pct': profit_margin_pct,
            'unit_price': unit_price,
            'unit_cost': unit_cost,
            'month': int(data['Month']),
            'day': int(data['Day']),
            'weekday': data['Weekday'],
            'season': season,
            'time_features': add_enhanced_time_features(data),
            'model_type': 'ethical_time_enhanced_forecasting',
            'price_ratio': price_ratio,
            'direct_revenue': True  # Flag to indicate this uses direct revenue approach
        }

    except Exception as e:
        # Log error and return error message
        import traceback
        traceback.print_exc()
        return {'error': str(e)}

def simulate_price_variations(base_data, min_price_factor=0.5, max_price_factor=2.0, steps=7):
    """
    Simulate revenue at different price points to analyze price elasticity.
    
    Parameters:
    - base_data: Base data with Unit Price, Unit Cost, Location, _ProductID, etc.
    - min_price_factor: Minimum factor to multiply base price by (default: 0.5)
    - max_price_factor: Maximum factor to multiply base price by (default: 2.0)
    - steps: Number of price points to simulate (default: 7)
    
    Returns:
    - List of dictionaries with simulation results
    """
    try:
        # Create a copy of the base data
        base_data = base_data.copy()
        
        # Check input arguments
        if min_price_factor <= 0:
            raise ValueError("min_price_factor must be positive")
        if max_price_factor <= min_price_factor:
            raise ValueError("max_price_factor must be greater than min_price_factor")
        if steps < 2:
            raise ValueError("steps must be at least 2")
            
        # Get base price
        base_price = float(base_data['Unit Price'])
        
        # Check if we're aggregating all locations
        is_aggregating_locations = False
        if base_data.get('Location') == 'All' or base_data.get('aggregate_locations'):
            is_aggregating_locations = True
            
        # Generate price factors
        if steps == 2:
            price_factors = [min_price_factor, max_price_factor]
        else:
            price_factors = np.linspace(min_price_factor, max_price_factor, steps)
            
        # Simulate at each price point
        results = []
        
        # Log product info for debugging
        product_id = base_data.get('_ProductID')
        location = base_data.get('Location')
        print(f"Simulating price variations for Product {product_id} at {location} location")
        
        for factor in price_factors:
            # Create a copy of the base data
            sim_data = base_data.copy()
            
            # Set the price for this simulation
            sim_data['Unit Price'] = base_price * factor
            
            # Make a prediction
            prediction = predict_revenue(sim_data)
            
            # Check for errors
            if 'error' in prediction:
                print(f"Error simulating price factor {factor}: {prediction['error']}")
                continue
                
            # Extract results
            revenue = prediction['predicted_revenue']
            quantity = prediction['estimated_quantity']
            profit = prediction['profit']
            
            # Generate scenario name based on price factor
            if abs(factor - 1.0) < 0.01:
                scenario_name = "Current Price"
            elif factor < 1.0:
                percent_lower = int(round((1.0 - factor) * 100))
                scenario_name = f"{percent_lower}% Lower"
            else:
                percent_higher = int(round((factor - 1.0) * 100))
                scenario_name = f"{percent_higher}% Higher"
            
            # Add to results
            results.append({
                'price_factor': factor,
                'unit_price': sim_data['Unit Price'],
                'unit_cost': float(sim_data['Unit Cost']),
                'predicted_revenue': revenue,
                'revenue': revenue,  # Include both field names for compatibility
                'predicted_quantity': quantity,
                'quantity': quantity,  # Include both field names for compatibility
                'profit': profit,
                'locations_aggregated': prediction.get('locations_aggregated', False),
                'location_count': prediction.get('location_count', 1) if is_aggregating_locations else 1,
                'scenario': scenario_name,  # Add scenario name for frontend display
                'Scenario': scenario_name   # Include both capitalizations for compatibility
            })
            
        # Sort by price factor
        results.sort(key=lambda x: x['price_factor'])
        
        # Log first and last result for comparison
        if results:
            print(f"Price factor {results[0]['price_factor']:.2f}: Revenue=${results[0]['predicted_revenue']:.2f}, Quantity={results[0]['predicted_quantity']}")
            print(f"Price factor {results[-1]['price_factor']:.2f}: Revenue=${results[-1]['predicted_revenue']:.2f}, Quantity={results[-1]['predicted_quantity']}")
            
            # Check if quantities are the same across different price factors
            quantities = [r['predicted_quantity'] for r in results]
            if len(set(quantities)) == 1 and len(quantities) > 1:
                print("WARNING: All price variations resulted in the same quantity - price elasticity may not be working correctly")
        
        return results
    
    except Exception as e:
        # Print error for debugging
        import traceback
        traceback.print_exc()
        print(f"Error in simulate_price_variations: {str(e)}")
        return []

def optimize_price(base_data, metric='profit', min_price_factor=0.5, max_price_factor=2.0, steps=20):
    """
    Find the optimal price that maximizes revenue or profit using the ethical time-enhanced model.
    
    Parameters:
    - base_data: dict with base input features
    - metric: 'revenue' or 'profit' to optimize
    - min_price_factor: minimum price factor (e.g., 0.5 = 50% of base price)
    - max_price_factor: maximum price factor (e.g., 2.0 = 200% of base price)
    - steps: number of price points to evaluate
    
    Returns:
    - dict with optimal price, revenue, and quantity
    """
    try:
        # Generate variations
        variations = simulate_price_variations(
            base_data, 
            min_price_factor=min_price_factor,
            max_price_factor=max_price_factor,
            steps=steps
        )
        
        if not variations:
            return None
        
        # Find optimal variation
        if metric == 'revenue':
            # Optimize for revenue
            optimal = max(variations, key=lambda x: x['Predicted Revenue'])
        else:
            # Optimize for profit
            optimal = max(variations, key=lambda x: x['Profit'])
        
        # Extract optimal details
        optimal_price = optimal['Unit Price']
        optimal_revenue = optimal['Predicted Revenue']
        optimal_quantity = optimal['Predicted Quantity']
        optimal_profit = optimal['Profit']
        optimal_factor = optimal['price_factor']
        
        # Calculate percentage improvement
        base_price = float(base_data.get('Unit Price', 0))
        if base_price > 0:
            price_change_pct = ((optimal_price / base_price) - 1) * 100
        else:
            price_change_pct = 0
        
        # Get base prediction for comparison
        base_prediction = predict_revenue(base_data)
        base_revenue = base_prediction.get('predicted_revenue', 0)
        base_profit = base_prediction.get('profit', 0)
        
        if metric == 'revenue':
            improvement_pct = ((optimal_revenue / base_revenue) - 1) * 100 if base_revenue > 0 else 0
        else:
            improvement_pct = ((optimal_profit / base_profit) - 1) * 100 if base_profit > 0 else 0
        
        return {
            'optimal_price': round(optimal_price, 2),
            'optimal_revenue': round(optimal_revenue, 2),
            'optimal_quantity': optimal_quantity,
            'optimal_profit': round(optimal_profit, 2),
            'optimal_factor': round(optimal_factor, 2),
            'price_change_pct': round(price_change_pct, 2),
            'improvement_pct': round(improvement_pct, 2),
            'metric': metric,
            'base_price': base_price,
            'base_revenue': round(base_revenue, 2),
            'base_profit': round(base_profit, 2),
            'variations': variations
        }
        
    except Exception as e:
        print(f"Error in optimize_price: {str(e)}")
        return None

def simulate_annual_revenue(base_data, min_price_factor=0.5, max_price_factor=2.0, steps=7):
    """
    Simulate annual revenue projections (365 days) at different price points.
    
    This function creates comprehensive annual projections by forecasting daily sales
    over a full year for each price point, providing the total annual impact.
    
    Parameters:
    - base_data: Base data with Unit Price, Unit Cost, Location, _ProductID, etc.
    - min_price_factor: Minimum factor to multiply base price by (default: 0.5)
    - max_price_factor: Maximum factor to multiply base price by (default: 2.0)
    - steps: Number of price points to simulate (default: 7)
    
    Returns:
    - List of dictionaries with annual simulation results
    """
    try:
        # Create a copy of the base data
        base_data = base_data.copy()
        
        # Validate inputs
        if min_price_factor <= 0:
            raise ValueError("min_price_factor must be positive")
        if max_price_factor <= min_price_factor:
            raise ValueError("max_price_factor must be greater than min_price_factor")
        if steps < 2:
            raise ValueError("steps must be at least 2")
            
        # Get base price
        base_price = float(base_data['Unit Price'])
        
        # Generate price factors
        if steps == 2:
            price_factors = [min_price_factor, max_price_factor]
        else:
            price_factors = np.linspace(min_price_factor, max_price_factor, steps)
            
        # Simulate annual projections for each price point
        results = []
        
        # Define simulation days first
        simulation_days = 365
        
        print(f"Simulating annual revenue projections for Product {base_data.get('_ProductID')} at {base_data.get('Location')} ({steps} price points, {simulation_days} days)")
        
        for factor in price_factors:
            # Create a copy for this price simulation
            sim_data = base_data.copy()
            sim_data['Unit Price'] = base_price * factor
            
            # Initialize annual totals
            annual_revenue = 0
            annual_quantity = 0
            annual_profit = 0
            successful_predictions = 0
            from datetime import datetime, timedelta
            import calendar
            
            # Start from current date or provided date
            start_date = datetime(
                base_data.get('Year', datetime.now().year),
                base_data.get('Month', datetime.now().month),
                base_data.get('Day', datetime.now().day)
            )
            
            for day_offset in range(simulation_days):
                current_date = start_date + timedelta(days=day_offset)
                
                # Update date fields for this simulation
                day_sim_data = sim_data.copy()
                day_sim_data['Year'] = current_date.year
                day_sim_data['Month'] = current_date.month
                day_sim_data['Day'] = current_date.day
                day_sim_data['Weekday'] = current_date.strftime('%A')
                
                try:
                    # Make daily prediction
                    prediction = predict_revenue(day_sim_data)
                    
                    if 'error' not in prediction:
                        annual_revenue += prediction['predicted_revenue']
                        annual_quantity += prediction['estimated_quantity']
                        annual_profit += prediction['profit']
                        successful_predictions += 1
                        
                except Exception as e:
                    # Continue with other days if one fails
                    continue
            
            # Only include this price point if we got reasonable predictions  
            min_required = int(simulation_days * 0.8)  # At least 80% successful predictions
            if successful_predictions > min_required:
                # Generate scenario name
                if abs(factor - 1.0) < 0.01:
                    scenario_name = "Current Price"
                elif factor < 1.0:
                    percent_lower = int(round((1.0 - factor) * 100))
                    scenario_name = f"{percent_lower}% Lower"
                else:
                    percent_higher = int(round((factor - 1.0) * 100))
                    scenario_name = f"{percent_higher}% Higher"
                
                # Add annual results
                results.append({
                    'price_factor': factor,
                    'unit_price': sim_data['Unit Price'],
                    'unit_cost': float(sim_data['Unit Cost']),
                    'predicted_revenue': annual_revenue,
                    'revenue': annual_revenue,  # Include both field names for compatibility
                    'predicted_quantity': annual_quantity,
                    'quantity': annual_quantity,  # Include both field names for compatibility
                    'profit': annual_profit,
                    'name': scenario_name,
                    'scenario': scenario_name,
                    'is_annual': True,
                    'days_projected': successful_predictions,
                    'note': f"Annual projection based on {successful_predictions} days"
                })
                
                # Reduced logging for performance in annual projections
                pass
            else:
                print(f"Skipping price factor {factor:.2f} - insufficient successful predictions ({successful_predictions}/{simulation_days})")
        
        # Sort by price factor
        results.sort(key=lambda x: x['price_factor'])
        
        return results
        
    except Exception as e:
        print(f"Error in simulate_annual_revenue: {str(e)}")
        import traceback
        traceback.print_exc()
        return []

def predict_revenue_batch(batch_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Predict revenue for multiple data points using vectorized batch inference.
    
    This function provides massive performance improvements by processing all predictions
    in a single model.predict() call instead of individual calls. Ideal for forecasting
    scenarios where you need predictions for many products/dates.
    
    Handles "All" location by expanding to all available locations automatically.
    
    Args:
        batch_data (List[Dict[str, Any]]): List of input data dictionaries, each containing:
            - Unit Price (float): Product unit price
            - Unit Cost (float): Product unit cost
            - _ProductID (str/int): Product identifier
            - Location (str): Location name or "All" for all locations
            - Month (int): Month (1-12)
            - Day (int): Day of month (1-31)
            - Weekday (str): Day name (e.g., "Monday")
            - Year (int): Year
    
    Returns:
        List[Dict[str, Any]]: List of prediction results, each containing:
            - predicted_revenue (float): Predicted revenue amount
            - estimated_quantity (int): Estimated quantity sold
            - total_cost (float): Total cost for the transaction
            - profit (float): Estimated profit
            - profit_margin_pct (float): Profit margin percentage
            - input_index (int): Index of the input data for matching results
            - location (str, optional): Actual location used for "All" expansions
    
    Raises:
        ValueError: If batch_data is empty or contains invalid data
        FileNotFoundError: If model files are not found
        RuntimeError: If batch prediction fails
        
    Example:
        >>> batch_inputs = [
        ...     {"Unit Price": 150.0, "Unit Cost": 75.0, "_ProductID": "12", "Location": "North", ...},
        ...     {"Unit Price": 200.0, "Unit Cost": 100.0, "_ProductID": "24", "Location": "All", ...}
        ... ]
        >>> results = predict_revenue_batch(batch_inputs)
        >>> print(f"Processed {len(results)} predictions in single batch call")
    """
    try:
        if not batch_data or len(batch_data) == 0:
            raise ValueError("Batch data cannot be empty")
        
        # Load the model once for the entire batch
        model_data, encoders, reference_data = load_model()
        model = model_data['model']
        
        # Get available locations from encoders
        available_locations = list(encoders.get('Location', {}).classes_) if 'Location' in encoders else []
        
        # Get feature names
        feature_names = None
        if 'features' in model_data:
            feature_names = model_data['features']
        elif 'feature_names' in model_data:
            feature_names = model_data['feature_names']
        elif hasattr(model, 'feature_name_'):
            feature_names = model.feature_name_
        elif hasattr(model, 'feature_names_'):
            feature_names = model.feature_names_
        
        # Expand batch data to handle "All" locations
        expanded_batch_data = []
        original_indices = []
        
        for i, data in enumerate(batch_data):
            if data.get('Location') == 'All':
                # Expand to all available locations
                for location in available_locations:
                    expanded_data = data.copy()
                    expanded_data['Location'] = location
                    expanded_batch_data.append(expanded_data)
                    original_indices.append(i)
            else:
                # Keep as is
                expanded_batch_data.append(data)
                original_indices.append(i)
        
        # Process all inputs and collect them into a single DataFrame
        processed_batch = []
        valid_indices = []
        valid_original_indices = []
        
        for i, data in enumerate(expanded_batch_data):
            try:
                # Create a copy to avoid modifying original
                data_copy = data.copy()
                
                # Validate and convert input
                validated_data = validate_and_convert_input(data_copy)
                
                # Preprocess data
                processed = preprocess(validated_data, model_data, encoders, reference_data)
                
                # Select required features
                if feature_names:
                    # Get all available features
                    missing_features = set(feature_names) - set(processed.columns)
                    if missing_features:
                        for feature in missing_features:
                            processed[feature] = 0
                    
                    X_row = processed[feature_names]
                else:
                    # If no feature names are available, use all columns
                    X_row = processed
                
                # Add to batch
                processed_batch.append(X_row.iloc[0])  # Get the single row as Series
                valid_indices.append(i)
                valid_original_indices.append(original_indices[i])
                
            except Exception as e:
                # Skip invalid inputs but continue processing
                print(f"Warning: Skipping batch item {i} due to error: {str(e)}")
                continue
        
        if not processed_batch:
            raise ValueError("No valid inputs in batch after preprocessing")
        
        # Convert to DataFrame for batch prediction
        X_batch = pd.DataFrame(processed_batch)
        
        # Make batch prediction - THIS IS THE KEY OPTIMIZATION
        # Instead of hundreds of individual model.predict() calls, we make 1 batch call
        y_pred_log_batch = model.predict(X_batch)
        
        # Apply inverse log transformation to get actual revenue
        y_pred_batch = np.expm1(y_pred_log_batch)
        
        # Process results for each prediction
        individual_results = []
        for idx, (expanded_idx, original_idx, y_pred) in enumerate(zip(valid_indices, valid_original_indices, y_pred_batch)):
            try:
                expanded_data = expanded_batch_data[expanded_idx]
                original_data = batch_data[original_idx]
                
                # Get input values
                unit_price = float(expanded_data.get('Unit Price', 0))
                unit_cost = float(expanded_data.get('Unit Cost', 0))
                product_id = expanded_data.get('_ProductID')
                location = expanded_data.get('Location')
                
                # Get product reference price for elasticity
                product_avg_price = None
                if product_id is not None:
                    product_id = int(product_id)
                    
                if 'product_price_avg' in reference_data and product_id in reference_data['product_price_avg']:
                    product_avg_price = reference_data['product_price_avg'][product_id]
                else:
                    # Use all products average as fallback
                    if 'product_price_avg' in reference_data:
                        product_avg_price = sum(reference_data['product_price_avg'].values()) / len(reference_data['product_price_avg'])
                    else:
                        product_avg_price = 5000.0  # Realistic fallback
                
                # Apply enhanced price elasticity
                price_ratio = unit_price / product_avg_price if product_avg_price > 0 else unit_price / 100.0
                
                # Apply elasticity adjustments
                if price_ratio > 1.0:
                    # Higher than average price - apply elasticity
                    elasticity_factor = np.exp(-0.5 * (price_ratio - 1.0))
                    y_pred = y_pred * elasticity_factor
                    
                    # For very high prices, apply even stronger elasticity
                    if price_ratio > 3.0:
                        additional_factor = np.exp(-1.0 * (price_ratio - 3.0))
                        y_pred = y_pred * additional_factor
                
                # For extreme prices, adjust prediction
                if unit_price > 100000:
                    y_pred = 0
                
                # Calculate quantity - PURE ML PREDICTION, NO ARTIFICIAL ROUNDING
                if unit_price > 0:
                    predicted_quantity = y_pred / unit_price
                else:
                    predicted_quantity = 0
                
                # KEEP ORIGINAL ML REVENUE PREDICTION
                adjusted_revenue = y_pred
                
                # Calculate derived metrics
                total_cost = predicted_quantity * unit_cost
                profit = adjusted_revenue - total_cost
                profit_margin_pct = (profit / adjusted_revenue) * 100 if adjusted_revenue > 0 else 0
                
                # Create result
                result = {
                    'predicted_revenue': float(adjusted_revenue),
                    'estimated_quantity': float(predicted_quantity),  # Keep decimal precision
                    'total_cost': float(total_cost),
                    'profit': float(profit),
                    'profit_margin_pct': float(profit_margin_pct),
                    'price_ratio': float(price_ratio),
                    'input_index': original_idx,
                    'location': location,
                    'was_all_location': original_data.get('Location') == 'All'
                }
                
                individual_results.append(result)
                
            except Exception as e:
                # Skip this result but continue processing
                print(f"Warning: Error processing batch result {idx}: {str(e)}")
                continue
        
        # Aggregate results for "All" location inputs
        final_results = []
        processed_indices = set()
        
        for original_idx in range(len(batch_data)):
            if original_idx in processed_indices:
                continue
                
            original_data = batch_data[original_idx]
            
            if original_data.get('Location') == 'All':
                # Aggregate all location results for this input
                location_results = [r for r in individual_results if r['input_index'] == original_idx and r['was_all_location']]
                
                if location_results:
                    # Sum up all location results
                    total_revenue = sum(r['predicted_revenue'] for r in location_results)
                    total_quantity = sum(r['estimated_quantity'] for r in location_results)
                    total_cost = sum(r['total_cost'] for r in location_results)
                    total_profit = sum(r['profit'] for r in location_results)
                    
                    # Calculate aggregated profit margin
                    profit_margin_pct = (total_profit / total_revenue) * 100 if total_revenue > 0 else 0
                    
                    # Use average price ratio
                    avg_price_ratio = sum(r['price_ratio'] for r in location_results) / len(location_results)
                    
                    aggregated_result = {
                        'predicted_revenue': float(total_revenue),
                        'estimated_quantity': float(total_quantity),  # Keep decimal precision
                        'total_cost': float(total_cost),
                        'profit': float(total_profit),
                        'profit_margin_pct': float(profit_margin_pct),
                        'price_ratio': float(avg_price_ratio),
                        'input_index': original_idx,
                        'locations_aggregated': True,
                        'location_count': len(location_results)
                    }
                    
                    final_results.append(aggregated_result)
                    processed_indices.add(original_idx)
            else:
                # Single location result
                location_results = [r for r in individual_results if r['input_index'] == original_idx and not r['was_all_location']]
                
                if location_results:
                    # Should be exactly one result
                    result = location_results[0]
                    # Remove internal fields
                    result.pop('was_all_location', None)
                    result.pop('location', None)
                    final_results.append(result)
                    processed_indices.add(original_idx)
        
        return final_results
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"Batch prediction failed: {str(e)}")

# Test the module if run directly
if __name__ == "__main__":
    # Test prediction
    test_data = {
        'Unit Price': 100.00,
        'Unit Cost': 50.00,
        'Month': 6,
        'Day': 15,
        'Weekday': 'Friday',
        'Location': 'North',
        '_ProductID': '1',
        'Year': 2023
    }
    
    print("Testing ethical time-enhanced prediction...")
    result = predict_revenue(test_data)
    print(f"Prediction result: {result}")
    
    # Test price simulation
    print("\nTesting price simulation...")
    variations = simulate_price_variations(test_data)
    for v in variations:
        print(f"Price: ${v['Unit Price']:.2f}, Quantity: {v['Predicted Quantity']}, Revenue: ${v['Predicted Revenue']:.2f}, Profit: ${v['Profit']:.2f}")
    
    # Test price optimization
    print("\nTesting price optimization for revenue...")
    optimal_revenue = optimize_price(test_data, metric='revenue')
    if optimal_revenue:
        print(f"Optimal price for revenue: ${optimal_revenue['optimal_price']:.2f}")
        print(f"Resulting revenue: ${optimal_revenue['optimal_revenue']:.2f}")
        print(f"Improvement: {optimal_revenue['improvement_pct']:.2f}%")
    
    print("\nTesting price optimization for profit...")
    optimal_profit = optimize_price(test_data, metric='profit')
    if optimal_profit:
        print(f"Optimal price for profit: ${optimal_profit['optimal_price']:.2f}")
        print(f"Resulting profit: ${optimal_profit['optimal_profit']:.2f}")
        print(f"Improvement: {optimal_profit['improvement_pct']:.2f}%") 